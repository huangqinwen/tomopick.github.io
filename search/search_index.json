{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>MiLoPYP is an open source, dataset-specific contrastive learning-based framework that enables two-step fast molecular pattern visualization followed by accurate protein localization without the need of any human annotation. During the exploration step, it learns an embedding space for 3D macromolecules such that similar structures are grouped together while dissimilar ones are separated. The embedding space is then projected into 2D and 3D which allows easy identification of the distribution of macromolecular structures across an entire dataset. During the refinement step, examples of proteins identified during the exploration step are selected and MiLoPYP learns to localize these proteins with high accuracy. </p> <p>Each step can be used separately. If to use refinement step only (tomogram particle detection), ground truth training coordinates need to be provided. Typically, around 200 coordinates from several tomograms are needed to ensure performance. Training coordinates can be obtained either through exploration step or manually. </p>"},{"location":"#installation","title":"Installation","text":"<p>The code was tested on CentOS Stream version 8.0, with Anaconda Python 3.8 and PyTorch version 1.11.0 and cu 10.2. NVIDIA GPUs with 32GB RAMs are needed for training and inference can be performed on either GPU or CPU. </p> <p>After install Anaconda: </p> <ol> <li> <p>[Optional but recommended] create a new conda environment. </p> <pre><code>conda create --name TomoPick python=3.8\n</code></pre> <p>And activate the environment.</p> <pre><code>conda activate TomoPick\n</code></pre> </li> <li> <p>Clone this repo:</p> <pre><code>TomoPick_ROOT=/path/to/clone/TomoPick \n</code></pre> <pre><code>git clone https://github.com/$TomoPick_ROOT\n</code></pre> </li> <li> <p>Install the requirements</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Install pytorch</p> <pre><code>pip install torch==1.11.0+cu102 torchvision==0.12.0+cu102 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu102\n</code></pre> </li> <li> <p>Install TomoPick package and dependencies (If shows error, try go to one level above): </p> <pre><code>pip install -e cet_pick\n</code></pre> </li> </ol> <pre><code>\u251c\u2500\u2500 data # data folder that stores all training data, can rename it if needed\n\u2502   \u251c\u2500\u2500 sample_train_explore_img.txt\n\u2502   \u251c\u2500\u2500 sample_train_refine_img.txt\n\u2502   \u251c\u2500\u2500 training_coordinates.txt\n\u251c\u2500\u2500 datasets # datasets fodler that contains all dataloading, sampling related code \n\u2502   \u251c\u2500\u2500 dataset_factory.py # file that contains all dataset factory and sampling factory \n\u2502   \u251c\u2500\u2500 tomo_*.py # data factory for different modes\n\u2502   \u251c\u2500\u2500 particle_*.py # sampling factory for different modes\n\u251c\u2500\u2500 trains # data fodler that stores all model training modules \n\u251c\u2500\u2500 models # model folder that contains all model architectures for different modes \n\u251c\u2500\u2500 utils # utils folder that contains all util functions \n\u251c\u2500\u2500 colormap # folder that contains colormaps for 2D visualization plot display  \n\u2514\u2500\u2500 DCNv2 # folder that contains deformable convolution related operations \n\u251c\u2500\u2500 opts.py # arguments for training\n\u251c\u2500\u2500 main.py # training file for refinement module \n\u251c\u2500\u2500 simsiam_main.py # training file for cellular content exploration module \n\u251c\u2500\u2500 simsiam_test_hm_3d.py # inference file for cellular content exploration module  \n\u251c\u2500\u2500 test.py # inference file for refinement/particle detection module \n\u251c\u2500\u2500 interactive_to_training_coords.py # convert output from interactive session to training coordinates file for refinement\n\u251c\u2500\u2500 plot_2d.py # plot 2D visualization plot, generate colors used for 3D tomogram visualization, and parquet file for interactive session  \n\u251c\u2500\u2500 phoenix_visualization.py # launch interactive session \n</code></pre>"},{"location":"#folder-structure","title":"Folder structure","text":""},{"location":"#sample-dataset-for-globular-shaped-protein","title":"Sample dataset for globular shaped protein","text":"<p>Please find sample dataset of EMPIAR-10304 (including <code>.rec</code>, <code>.tlt</code>, <code>.ali</code>, parquet file from interactive session, sample train img txt for both exploration and refinement module, sample train coordinates txt for refinement module) here. For usage, download all files and save all <code>.rec</code>, <code>.tlt</code>, <code>.ali</code> files to <code>sample_data</code> directory, save all <code>.txt</code> files to <code>data</code> directory. </p>"},{"location":"#sample-dataset-for-tubular-shaped-protein","title":"Sample dataset for tubular shaped protein","text":"<p>Please find sample dataset of EMPIAR-10987 (including <code>.rec</code>, sample train images and coordinates txt for refinement module) here. For usage, download all files and save all <code>.rec</code>, files to <code>sample_data_microtubule</code> directory, save all <code>.txt</code> files to <code>data</code> directory. </p>"},{"location":"explore/","title":"Cellular Content Exploration","text":""},{"location":"explore/#cellular-content-exploration-module","title":"Cellular content exploration module","text":"<p>For the exploration step, MiLoPYP learns an embedding space for 3D macromolecules such that similar structures are grouped together while dissimilar ones are separated. The embedding space is then projected into 2D and 3D which allows easy identification of the distribution of macromolecular structures across an entire dataset. </p>"},{"location":"explore/#input-preparation","title":"Input preparation","text":"<p>There are two different mode for this module: </p> <p><code>2d3d</code>: Input to this module include aligned 2D tilt series (.mrc, or .ali as in <code>sample_data</code>), corresponding angles (.tlt), 3D reconstructed tomograms from tilt series (.rec). </p> <p><code>3d</code>: Input to this module is 3D reconstructed tomograms(.rec) alone. </p> <p>For <code>2d3d</code> mode, the training file should be a <code>.txt</code> file with tomogram name, path to 2D tilt series, path to corresponding angles, path to 3D reconstructed tomograms in the following format: </p> <pre><code>image_name   rec_path    tilt_path   angle_path  \n\ntomo1   path_to_rec_1   path_to_tilt_1   path_to_angle_1\n\ntomo2   path_to_rec_2   path_to_tilt_2   path_to_angle_2\n...\n</code></pre> <p>For example, suppose we store tilt series, corresponding angles, and reconstructed tomograms to the data directory with path: <code>/data</code>, the training txt file will look like this:</p> <pre><code>image_name   rec_path    tilt_path   angle_path  \n\ntomo1   sample_data/tomo1.rec   sample_data/tomo1.mrc   sample_data/tomo1.tlt\n\ntomo2   sample_data/tomo2.rec   sample_data/tomo1.mrc   sample_data/tomo1.tlt\n...\n</code></pre> <p>For <code>3d</code> mode, the training txt file only need <code>rec_path</code> and tomogram name. <code>tilt_path</code> and <code>angle_path</code> are not needed. Therefore, the file will have the following format: </p> <p><pre><code>image_name   rec_path     \n\ntomo1   path_to_rec_1   \n\ntomo2   path_to_rec_2   \n...\n</code></pre> It is also ok to use the same file formatted for <code>2d3d</code> mode. Only <code>rec_path</code> will be used. </p> <p>Warning</p> <p>Make sure the 2D tilt seires are aligned tilt series. It also needs to be the same <code>x-y</code> dimension as 3D tomogram. Typically we recommend using downsampled tilt series and 3D tomograms (with x-y dimension less than 2000px)</p> <p>Once files are generated, move all training files to <code>data/</code> directory (create <code>data/</code> directory if it doesn't exist)</p>"},{"location":"explore/#training","title":"Training","text":"<p>To train exploration module in 2d3d mode (with tilt series and tomograms), run: <pre><code>python simsiam_main.py simsiam2d3d --num_epochs 300 --exp_id test_sample --bbox 36 --dataset simsiam2d3d --arch simsiam2d3d_18 --lr 1e-3 --train_img_txt sample_train_explore_img.txt --batch_size 256 --val_intervals 20 --save_all --gauss 0.8 --dog 3,5\n</code></pre> For <code>2d3d</code> mode, training-related files will be saved to <code>exp/simsiam2d3d/test_sample</code>, including log file with loss and all used arguments. </p> <p>To train exploration module in 3d mode (using tomogram only), run:</p> <p><pre><code>python simsiam_main.py simsiam3d --num_epochs 300 --exp_id test_sample --bbox 36 --dataset simsiam3d --arch simsiam2d_18 --lr 1e-3 --train_img_txt sample_train_explore_img.txt --batch_size 256 --val_intervals 20 --save_all --gauss 0.8 --dog 3,5\n</code></pre> For <code>3d</code> mode, training-related files will be saved to <code>exp/simsiam3d/test_sample</code>, including log file with loss and all used arguments. </p> Arguments Purpose <code>num_epochs</code> number of training epochs, recommend 100 to 300 <code>exp_id</code> experiment id you want to save it as. <code>bbox</code> bounding box size for cropped patches, should bigger than particles <code>dataset</code> sampling and dataloader mode <code>arch</code> model backbone architecture <code>lr</code> learning rate <code>training_img_txt</code> input txt for training <code>batch_size</code> batch size for training <code>val_intervals</code> save model every val_intervals <code>save_all</code> whether to save all models for each val_interval <code>gauss</code> preprocessing gaussian filter to denoise tilt series and tomogram <code>dog</code> kernel sizes for difference of gaussian pyramid, comma delimited <p>For more information regarding arguments, go to <code>opts.py</code></p>"},{"location":"explore/#inference","title":"Inference","text":"<p>After training, to map tomograms/tilt series into embeddings, use trained model to perform mapping.</p> <p>For <code>2d3d</code> mode, run:  <pre><code>python simsiam_test_hm_2d3d.py simsiam2d3d --exp_id test_sample --bbox 36 --dataset simsiam2d3d --arch simsiam2d3d_18 --test_img_txt sample_train_explore_img.txt --load_model exp/simsiam2d3d/test_sample/model_300.pth --gauss 0.8 --dog 3,5\n</code></pre></p> <p>For <code>3d</code> mode, run:</p> <p><pre><code>python simsiam_test_hm_3d.py simsiam3d --exp_id test_sample --bbox 36 --dataset simsiam3d --arch simsiam2d_18 --test_img_txt sample_train_explore_img.txt --load_model exp/simsiam3d/test_sample/model_300.pth --gauss 0.8 --dog 3,5\n</code></pre> Here we are using trained model from 300<sup>th</sup> epoch. </p> Note: Please make sure to use same architecture, bounding box size, gauss, and dog argument for both training and inference and select proper trained model <p>For example, if use <code>--bbox 36 --gauss 0.8 --dog 3,5</code> in training arguments, make sure the same arguments are used for inference.  To find used arguments for training, go to the output folder and check <code>opts.txt</code>. For trained model selection, check loss in <code>log.txt</code> and select models with lower loss. </p> <p>Inference output is a npz file that contains embeddings, corresponding coordinates, original cropped patches from tomogram, names of corresponding tomogram. Output is saved to <code>exp/simsiam2d3d/test_sample/all_output_info.npz</code> for <code>2d3d</code> mode and <code>exp/simsiam3d/test_sample/all_output_info.npz</code>for 3d mode. </p>"},{"location":"explore/#2d-visualization-plot","title":"2D visualization plot","text":"2D visualization plot2D visualization with labels <p>To generate 2D visualization plot, run: <pre><code>python plot_2d.py --input exp/simsiam2d3d/test_sample/all_output_info.npz --n_cluster 48 --num_neighbor 40 --mode umap --path exp/simsiam2d3d/test_sample/ --min_dist_vis 1.3e-3 \n</code></pre> 2D visualization plot will be saved to <code>exp/simsiam2d3d/test_sample/2d_visualization_out.png</code> and an additional visualization plot with labels generated using over-clustering algorithm <code>exp/simsiam2d3d/test_sample/2d_visualization_labels.png</code>, in the same directory, additional outputs include <code>all_colors.npy</code> that will be used as input for 3D tomogram visualization plotting and <code>interactive_info_parquet.gzip</code> that includes labels from overclustering and is used as input for interactive session. PNGs of all cropped patches will be also saved to <code>exp/simsiam2d3d/test_sample/imgs/</code> folder. This will later be used for interactive session. For <code>3d</code> mode, replace <code>simsiam2d3d</code> with <code>simsiam3d</code>.</p> Arguments Purpose <code>input</code> output all_output_info.npz from inference <code>n_cluster</code> number of clusters for overclustering <code>num_neighbor</code> number of neighbors for both tsne and umap clustering <code>mode</code> whether to use tsne or umap for dimensionality reduction <code>path</code> path of directory to save all output and images <code>host</code> local host for images, default is 7000 <code>min_dist_umap</code> min distance in UMAP <code>min_dist_vis</code> min distance for patch display on2d visualization"},{"location":"explore/#3d-tomogram-visualization","title":"3D Tomogram visualization","text":"<p>To generate 3D tomogram visualization plot, run: <pre><code>python visualize_3dhm.py --input exp/simsiam2d3d/test_sample/all_output_info.npz --color exp/simsiam2d3d/test_sample/all_colors.npy --dir_simsiam exp/simsiam2d3d/test_sample/ --rec_dir sample_data/\n</code></pre> Outputs include two numpy arrays: <code>*rec3d.npy</code> is the numpy array for 3D tomogram and <code>*hm3d_simsiam.npy</code> is  the numpy array for color heatmaps. To generate example visualization plot, two arrays can be </p> <ol> <li> <p>loaded using napari as two layers and transparency of each layer can be adjusted in the napari interface</p> </li> <li> <p>blending two arrays by weighted averaging <code>w x rec_3d.npy + (1-w) x hm3d_simsiam.npy</code> and the resulting array will be the 3D tomogram visualization with color. </p> </li> </ol> Arguments Purpose <code>input</code> output all_output_info.npz from inference <code>color</code> .npy color array, generated by plot_2d.py <code>dir_simsiam</code> directory to the current run <code>rec_dir</code> path to directory with corresponding rec file <p>For <code>3d</code> mode, replace <code>simsiam2d3d</code> with <code>simsiam3d</code>.</p>"},{"location":"explore/#3d-interactive-session","title":"3D interactive session","text":"<p>Interactive session requires loading of local images (which are already generated by <code>plot_2d.py</code>). Local images are generated by <code>plot_2d.py</code> under directory <code>exp/simsiam2d3d/test_sample/imgs/</code>. To connect images to local host and keep it running in the background, initiate a new screen session in terminal with <code>screen</code>, go to <code>exp/simsiam2d3d/test_sample/</code> and run <code>python -m http.server 7000</code>. The images will be hosted on <code>7000</code>. Detach the screen session. </p> <p>Warning</p> <p>Make sure to use the same number as the <code>host</code> argument in <code>plot_2d.py</code>, default is 7000. </p> <p>To initiate interactive session, run: <pre><code>python phoenix_visualization.py --input exp/simsiam2d3d/test_sample/interactive_info_parquet.gzip\n</code></pre> On terminal, it should show headers of parquet file, including local host address for interactive session. For example, here the local host address is <code>http://localhost:33203/</code> </p> <p>You should now be able to access the interactive session at <code>http://localhost:33203/</code>.</p> <p>Warning</p> <p>If you are running everything on a remote cluster and want to visualize everything through local browser, you will first need to connect remote to local. This needs to be done for both images and the interactive session. To connect images with local host 7000, use <code>ssh -N -f -L localhost:7000:localhost:7000 your_remote_login_address</code> on local terminal. To connect interactive session with local host 33203, use  <code>ssh -N -f -L localhost:33203:localhost:33203 your_remote_login_address</code>.</p> <p>In the interactive session, you should be able to visualize clusters of 3D embeddings, you will be able to adjust the number of points to be displayed in the cluster, coloring of each embedding based on label, select subclusters based on labels, and export selected subclusters. </p> homepage for interactive sessionadjust number of points to be displayedcoloring of embeddings based on labelsselect and export subclusters based on labels <p></p> <p></p> <p></p> <p></p>"},{"location":"explore/#convert-exported-parquet-files-to-training-coordinates-file-for-refinement-module","title":"Convert exported parquet files to training coordinates file for refinement module","text":"<p>Exported coordinates can be downloaded through interactive session. In the example below, the downloaded parquet is named as <code>2023-10-08_19-44-41.parquet</code>.</p> <p></p> <p>Convert parquet to training coordinates <code>.txt</code> files by running: <pre><code>python interactive_to_training_coords.py --input path_to_dir_of_parquets --output training_coordinates.txt \n</code></pre></p> Arguments Purpose <code>input</code> full path to directory that contains all downloaded parquet file <code>output</code> full path to output training coordinates file <code>if_double</code> whether z coordinates obtained from DoGs in exploration module is downscaled by 2, if so needs to rescale it back <p>Now, we can using the same training image file <code>sample_train_explore_img.txt</code> and the generated trainning coordinates file <code>training_coordinates.txt</code> for the subsequent refinement module.</p>"},{"location":"quick_tutorial/","title":"Quick tutorial","text":""},{"location":"quick_tutorial/#quick-tutorial-on-globular-shaped-data","title":"Quick tutorial on globular shaped data","text":"<p>Here is a quick tutorial on the sample globular shaped data here. Sample data includes the following files:</p> <ul> <li>tiltx.rec: 3D downsampled reconstructed tomograms with size 512x512x256</li> <li>tiltx.ali: aligned downsampled tilt series with size 512x512x41</li> <li>tiltx.tlt: tilt angles </li> <li>2023-10-08_19-44-41.parquet: output from interactive session that includes selected coordinates for refinement module training</li> <li>sample_train_explore_img.txt: train image txt for exploration module and refinement module </li> <li>training_coordinates.txt train coordinates txt for refinement module, converted from parquet file above. </li> </ul> <p>Once downloaded, go to the same folder as <code>main.py</code> and <code>test.py</code>. Create a folder for <code>.rec</code>, <code>.ail</code> and <code>.tlt</code> files and name it as <code>sample_data</code>. Create another folder and name it as <code>data</code> for <code>.txt</code> files. </p> <pre><code>\u251c\u2500\u2500 data \n\u2502   \u251c\u2500\u2500 sample_train_explore_img.txt\n\u2502   \u251c\u2500\u2500 training_coordinates.txt\n\u251c\u2500\u2500 sample_data\n\u2502   \u251c\u2500\u2500 *.rec\n\u2502   \u251c\u2500\u2500 *.ali\n\u2502   \u251c\u2500\u2500 *.tlt\n\u251c\u2500\u2500 main.py \n\u251c\u2500\u2500 test.py \n</code></pre>"},{"location":"quick_tutorial/#cellular-content-exploration","title":"Cellular content exploration","text":"<p>To start training, run:</p> <pre><code>python simsiam_main.py simsiam2d3d --num_epochs 300 --exp_id test_sample --bbox 36 --dataset simsiam2d3d --arch simsiam2d3d_18 --lr 1e-3 --train_img_txt sample_train_explore_img.txt --batch_size 256 --val_intervals 20 --save_all --gauss 0.8 --dog 3,5\n</code></pre> <p>Outputs include logged loss for each epoch, trained model at each 20 intervals and logged opts. </p> <p>Once trained, to map tomograms/tilt series into embeddings, run:</p> <pre><code>python simsiam_test_hm_2d3d.py simsiam2d3d --exp_id test_sample --bbox 36 --dataset simsiam2d3d --arch simsiam2d3d_18 --test_img_txt sample_train_explore_img.txt --load_model exp/simsiam2d3d/test_sample/model_300.pth --gauss 0.8 --dog 3,5\n</code></pre> <p>Go to <code>exp/simsiam2d3d/test_sample/</code>, output is a npz file - <code>all_output_info.npz</code> that contains embeddings, corresponding coordinates, original cropped patches from tomogram, names of corresponding tomogram.</p> <p></p> <ul> <li>2D visualization generation:</li> </ul> <pre><code>python plot_2d.py --input exp/simsiam2d3d/test_sample/all_output_info.npz --n_cluster 48 --num_neighbor 40 --mode umap --path exp/simsiam2d3d/test_sample/ --min_dist_vis 1.3e-3 \n</code></pre> <ul> <li>3D tomogram visualization:</li> </ul> <pre><code>python visualize_3dhm.py --input exp/simsiam2d3d/test_sample/all_output_info.npz --color exp/simsiam2d3d/test_sample/all_colors.npy --dir_simsiam exp/simsiam2d3d/test_sample/ --rec_dir sample_data/\n</code></pre> <ul> <li>3D interactive session:</li> </ul> <p>To launch, run:</p> <pre><code>python phoenix_visualization.py --input exp/simsiam2d3d/test_sample/interactive_info_parquet.gzip\n</code></pre> <p>To convert downloaded parquet files from interactive sessions, run:</p> <p><pre><code>python interactive_to_training_coords.py --input path_to_all_parquet_files --output training_coordinates.txt \n</code></pre> Note: <code>--input</code> should be the path that contains all parquet files not a single parquet file.</p> <p>The obtained <code>training_coordinates.txt</code> should be the same as downloaded <code>training_coordinates.txt</code>. </p>"},{"location":"quick_tutorial/#refined-particle-localization","title":"Refined particle localization","text":"<p>To train model for refined particle localization, run:</p> <pre><code>python main.py semi --down_ratio 2 --num_epochs 10 --bbox 16 --exp_id sample_refinement --dataset semi --arch unet_4 --save_all --debug 4 --val_interval 1 --thresh 0.85 --cr_weight 0.1 --temp 0.07 --tau 0.01 --lr 5e-4 --train_img_txt sample_train_explore_img.txt --train_coord_txt training_coordinates.txt --val_img_txt sample_train_explore_img.txt --val_coord_txt training_coordinates.txt --K 900 --compress --order xzy --gauss 0.8 --contrastive --last_k 3\n</code></pre> <p>To run inference using trained model, run:</p> <pre><code>python test.py semi --arch unet_4 --dataset semi --exp_id sample_refinement --load_model exp/semi/sample_refinement/model_4.pth --down_ratio 2 --K 900 --ord xzy --out_thresh 0.25 --test_img_txt test_img.txt --compress --gauss 0.8 --out_id all_out\n</code></pre> <p>Please find all coordinates output in <code>exp/semi/sample_refinement/all_out/*.txt</code></p>"},{"location":"quick_tutorial_tub/","title":"Quick tutorial tub","text":""},{"location":"quick_tutorial_tub/#quick-tutorial-on-globular-shaped-data","title":"Quick tutorial on globular shaped data","text":"<p>Here is a quick tutorial on the sample globular shaped data here. Sample data includes the following files:</p> <ul> <li>tiltx.rec: 3D downsampled reconstructed tomograms with size 512x512x256</li> <li>tiltx.ali: aligned downsampled tilt series with size 512x512x41</li> <li>tiltx.tlt: tilt angles </li> <li>2023-10-08_19-44-41.parquet: output from interactive session that includes selected coordinates for refinement module training</li> <li>sample_train_explore_img.txt: train image txt for exploration module and refinement module </li> <li>training_coordinates.txt train coordinates txt for refinement module, converted from parquet file above. </li> </ul> <p>Once downloaded, go to the same folder as <code>main.py</code> and <code>test.py</code>. Create a folder for <code>.rec</code>, <code>.ail</code> and <code>.tlt</code> files and name it as <code>sample_data</code>. Create another folder and name it as <code>data</code> for <code>.txt</code> files. </p> <pre><code>\u251c\u2500\u2500 data \n\u2502   \u251c\u2500\u2500 sample_train_explore_img.txt\n\u2502   \u251c\u2500\u2500 training_coordinates.txt\n\u251c\u2500\u2500 sample_data\n\u2502   \u251c\u2500\u2500 *.rec\n\u2502   \u251c\u2500\u2500 *.ali\n\u2502   \u251c\u2500\u2500 *.tlt\n\u251c\u2500\u2500 main.py \n\u251c\u2500\u2500 test.py \n</code></pre>"},{"location":"quick_tutorial_tub/#cellular-content-exploration","title":"Cellular content exploration","text":"<p>To start training, run:</p> <pre><code>python simsiam_main.py simsiam2d3d --num_epochs 300 --exp_id test_sample --bbox 36 --dataset simsiam2d3d --arch simsiam2d3d_18 --lr 1e-3 --train_img_txt sample_train_explore_img.txt --batch_size 256 --val_intervals 20 --save_all --gauss 0.8 --dog 3,5\n</code></pre> <p>Outputs include logged loss for each epoch, trained model at each 20 intervals and logged opts. </p> <p>Once trained, to map tomograms/tilt series into embeddings, run:</p> <pre><code>python simsiam_test_hm_2d3d.py simsiam2d3d --exp_id test_sample --bbox 36 --dataset simsiam2d3d --arch simsiam2d3d_18 --test_img_txt sample_train_explore_img.txt --load_model exp/simsiam2d3d/test_sample/model_300.pth --gauss 0.8 --dog 3,5\n</code></pre> <p>Go to <code>exp/simsiam2d3d/test_sample/</code>, output is a npz file - <code>all_output_info.npz</code> that contains embeddings, corresponding coordinates, original cropped patches from tomogram, names of corresponding tomogram.</p> <p></p> <ul> <li>2D visualization generation:</li> </ul> <pre><code>python plot_2d.py --input exp/simsiam2d3d/test_sample/all_output_info.npz --n_cluster 48 --num_neighbor 40 --mode umap --path exp/simsiam2d3d/test_sample/ --min_dist_vis 1.3e-3 \n</code></pre> <ul> <li>3D tomogram visualization:</li> </ul> <pre><code>python visualize_3dhm.py --input exp/simsiam2d3d/test_sample/all_output_info.npz --color exp/simsiam2d3d/test_sample/all_colors.npy --dir_simsiam exp/simsiam2d3d/test_sample/ --rec_dir sample_data/\n</code></pre> <ul> <li>3D interactive session:</li> </ul> <p>To launch, run:</p> <pre><code>python phoenix_visualization.py --input exp/simsiam2d3d/test_sample/interactive_info_parquet.gzip\n</code></pre> <p>To convert downloaded parquet files from interactive sessions, run:</p> <p><pre><code>python interactive_to_training_coords.py --input path_to_all_parquet_files --output training_coordinates.txt \n</code></pre> Note: <code>--input</code> should be the path that contains all parquet files not a single parquet file.</p> <p>The obtained <code>training_coordinates.txt</code> should be the same as downloaded <code>training_coordinates.txt</code>. </p>"},{"location":"quick_tutorial_tub/#refined-particle-localization","title":"Refined particle localization","text":"<p>To train model for refined particle localization, run:</p> <pre><code>python main.py semi --down_ratio 2 --num_epochs 10 --bbox 16 --exp_id sample_refinement --dataset semi --arch unet_4 --save_all --debug 4 --val_interval 1 --thresh 0.85 --cr_weight 0.1 --temp 0.07 --tau 0.01 --lr 5e-4 --train_img_txt sample_train_explore_img.txt --train_coord_txt training_coordinates.txt --val_img_txt sample_train_explore_img.txt --val_coord_txt training_coordinates.txt --K 900 --compress --order xzy --gauss 0.8 --contrastive --last_k 3\n</code></pre> <p>To run inference using trained model, run:</p> <pre><code>python test.py semi --arch unet_4 --dataset semi --exp_id sample_refinement --load_model exp/semi/sample_refinement/model_4.pth --down_ratio 2 --K 900 --ord xzy --out_thresh 0.25 --test_img_txt test_img.txt --compress --gauss 0.8 --out_id all_out\n</code></pre> <p>Please find all coordinates output in <code>exp/semi/sample_refinement/all_out/*.txt</code></p>"},{"location":"refine/","title":"Particle Refined Localization","text":""},{"location":"refine/#particle-refined-localization-module","title":"Particle refined localization module","text":"<p>For the refinement step, MiLoPYP learns to localize proteins of interests with high accuracy, when trained using sparsely annotated data. This step can be used without the previous exploration step. </p>"},{"location":"refine/#input-preparation","title":"Input preparation","text":"<p>Training set should include two files: 1. a <code>.txt</code> file with tomogram name and path to tomogram; 2. a <code>.txt</code> file with image name and its corresponding x,y,z coordinates.</p> <ul> <li> <p>If use refinement module after exploration module, we can use the same train image file and the generated train coordinates file from the exploration module. </p> </li> <li> <p>If use refinement module alone, some manual labeling is needed to generate training coordinates. A corresponding train image file is needed as well. </p> </li> </ul> <p>For 1, the text file should have the following format:</p> <pre><code>image_name   rec_path   \n\ntomo1   path_to_tomo1\n\ntomo2   path_to_tomo2 \n...\n</code></pre> <p>For 2, the text file should have the following format:</p> <pre><code>image_name  x_coord y_coord z_coord  \n\ntomo1   125 38  60 \n\ntomo1   130 45  80\n...\n</code></pre>"},{"location":"refine/#generate-training-set-from-manual-label","title":"Generate training set from manual label","text":"<p>We provide a simple code to generate the described training set from a selected folder. </p> <p>First, make a train directory for all training 3d tomograms and its corresponding coordinates files. Each micrograph should have its own coordinate file: e.g. train_1_img.rec and train_1_img_coord.txt </p> <p>For training coordinates, manually picking is performed on selected micrographs using imod. For a single tomogram, a full annotation is not required. Simply find some subregions and pick around 10\\% to 70\\% of the particles in that subregion. The subregion does not need to be big.  After manual annotation, imod will generate <code>.mod</code> files for annotated coordinates. Converting <code>.mod</code> files to <code>.txt</code> files can be done through <code>model2point</code> command from imod. A sample command is: <pre><code>model2point input.mod input.txt \n</code></pre> Once all the <code>.mod</code> files are converted to text files, move all coordinate.txt files to the train directory. </p> <p>Warning</p> <p>Depends on the x-y-z order of your input tomogram, the output coodinates generated using imod will be in different order. Two most common orders are <code>x-y-z</code> and <code>x-z-y</code>. Make sure you get the orders correct. </p> <p>Once all the <code>.mod</code> files are converted to text files, move all coordinate.txt files to the train directory. </p> <p>To generate train image and coordinate files, run <code>generate_train_files.py</code> file under <code>utils/</code> folder. Two input arguments are required: <code>-d/--dir</code> for path to train directory, and <code>-o/--out</code> for output training file name. The default order for input coordinates is <code>x-z-y</code>, if you want to specify a different order, add <code>-r/--ord</code>. Accepted orders are: xyz, xzy, zxy.  An example command:</p> <pre><code>python generate_train_files.py -e .rec -d sample_data -o sample_run -r xyz \n</code></pre> Arguments Purpose <code>ext</code> extension for tomogram files <code>dir</code> path that contains all training tomograms and labels <code>out</code> training file output name <code>ord</code> order of the tomogram <code>inference</code> if set to true, generating input for evaluation stage <p>Once files are generated, move all training files to <code>data/</code> directory (create <code>data/</code> directory if it doesn't exist)</p>"},{"location":"refine/#training","title":"Training","text":""},{"location":"refine/#globular-shaped-proteins","title":"Globular shaped proteins","text":"<p>A sample training command to train sample EMPIAR-10304 dataset (suppose train image txt is called <code>sample_train_explore_img.txt</code> and train coordinates txt is called <code>training_coordinates.txt</code> and validation image txt is called <code>sample_val_img.txt</code> and validation coordinates txt is called <code>val_coordinates.txt</code> (Validation files are optional):</p> <pre><code>python main.py semi --down_ratio 2 --num_epochs 10 --bbox 16 --exp_id sample_refinement --dataset semi --arch unet_5 --save_all --debug 4 --val_interval 1 --thresh 0.85 --cr_weight 0.1 --temp 0.07 --tau 0.01 --lr 5e-4 --train_img_txt sample_train_explore_img.txt --train_coord_txt training_coordinates.txt --val_img_txt sample_val_img.txt --val_coord_txt val_coordinates.txt --K 900 --compress --order xzy --gauss 0.8 --contrastive --last_k 3\n</code></pre> Arguments Purpose <code>num_epochs</code> number of training epochs, recommend 5 to 10 <code>exp_id</code> experiment id you want to save it as. <code>bbox</code> bounding box size for particle, used to generate Guassian kernel during training. <code>dataset</code> sampling and dataloader mode, use <code>semi</code> for default <code>arch</code> model backbone architecture, Format is <code>name\"_\"numOfLayers</code> - recommend unet_4 and unet_5 <code>lr</code> learning rate, recommend 1e-3 to 5e-4, less training examples, lower learning rate <code>debug</code> debug mode for visualization, currently only support mode 4 for easier visualization - output will be saved to 'debug folder' including view of each slice, ground truth heatmap, predicted heatmap, and detection prediction based on heatmap <code>val_interval</code> interval to perform validation and save model every val intervals <code>cr_weight</code> weight for contrastive regularization, recommend smaller number for more samples, larger number for less samples <code>save_all</code> whether to save all models for each val_interval <code>gauss</code> preprocessing gaussian filter to denoise tilt series and tomogram <code>temp</code> infoNCE temperature <code>down_ratio</code> downsampling in x-y direction, default is 2. Recommend 2 <code>tau</code> class prior probability <code>thresh</code> threshold for soft/hard positives <code>last_k</code> size of convolution filter for last layer <code>compress</code> whether to combine 2 z-slices into 1, recommend on <code>K</code> maximum number of particles <code>fiber</code> if the protein-of-interest is fiber/tubular shaped, turn this on <p>More description of arguments are in <code>opt.py</code> file.  Please refer to the paper for detailed parameter selection.</p> <p>All outputs will be saved to <code>exp/semi/exp_id</code> folder. For this command, outputs will be saved to <code>exp/semi/sample_refinement</code>. Within folder, there will be: </p> <ul> <li><code>opt.txt</code> which saves all the option you used. </li> <li><code>debug</code> folder, which saves all validation output. </li> <li><code>model_xxx.pth</code> model checkpoint, the final model weights will be <code>modelxxx_last_contrastive.pth</code>. </li> <li>A directory with specific training/validation loss info for each run </li> </ul> <p>Sample outputs in <code>debug</code> folder</p> predicted heatmappredicted output after nms <p></p> <p></p> <p>How to select which model to use and cutoff threshold score for output?</p> <p>Model and cutoff  threshold score can be selected based on validation loss and outputs in <code>debug</code> folder. </p> Model selectionCutoff Threshold Selection <p>When there is fully labeled tomogram for validation, typically select the model with the lowest validation loss.  When there is no fully labeled tomogram, select the model that generates best heatmaps.  Typically, unless there's severe overfitting, model from the last epoch generates good results. </p> <p>Cutoff threshold selection can be estimated based on detection output (a <code>.txt</code> file that contains x,y,z coordinates and corresponding detection score). It can also be estimated from <code>*_pred_out.png</code> images within <code>debug</code> fodler that circles out identified particles above a certain cutoff threshold. If there are many false positives, consider using a higher cutoff score. </p>"},{"location":"refine/#training-on-tubular-shaped-proteins","title":"Training on tubular shaped proteins","text":"<p>A sample training command to train sample EMPIAR-10987 dataset (suppose train image txt is called <code>sample_train_microtubule_img.txt</code> and train coordinates txt is called <code>training_coordinates_microtubule.txt</code> and validation image txt is called <code>sample_val_microtubule.txt</code> and validation coordinates txt is called <code>val_coordinates_microtubule.txt</code> (Validation files are optional):</p> <pre><code>python main.py semi --down_ratio 2 --num_epochs 10 --bbox 12 --contrastive --exp_id fib_test --dataset semi --arch unet_5 --save_all --debug 4 --val_interval 1 --thresh 0.3 --cr_weight 1.0 --temp 0.07 --tau 0.01 --lr 1e-4 --train_img_txt sample_train_microtubule_img.txt --train_coord_txt training_coordinates_microtubule.txt --val_img_txt sample_val_microtubule.txt --val_coord_txt val_coordinates_microtubule.txt --K 550 --compress --gauss 1 --order xzy --last_k 5 --fiber\n</code></pre> <p>Note here the main different is the <code>--fiber</code> command. Since the protein-of-interest is of tubular shape, make sure to have <code>--fiber</code> turned on. </p> <p>Outputs will be the same as training for globular shaped proteins. </p> <p>Sample outputs in <code>debug</code> folder. </p> predicted microtubule heatmappredicted microtubule output after nms without post processing <p>[</p> <p></p>"},{"location":"refine/#training-on-gpus-with-less-memory-using-lightweight-model","title":"Training on GPUs with less memory using lightweight model","text":"<p>For GPUs with less than 32GB RAM (e.g., 16GB), we provide a lightweight model that employs a modified contrastive learning approach. This model achieves comparable particle picking results while using less memory and offering improved time efficiency. Additionally, it demonstrates enhanced performance in spike detection. The lightweight model uses the same input training files but requires different command-line arguments. Below is a sample command for training the lightweight model:</p> <p><pre><code>python main_class.py semiclass --num_epochs 20 --bbox 16 --contrastive --exp_id sample_lightweight --arch small_5 --debug 4 --val_interval 2 --thresh 0.5 --cr_weight 0.1 --temp 0.07 --tau 0.01 --lr 1e-3 --train_img_txt sample_train_explore_img.txt --train_coord_txt training_coordinates.txt --val_img_txt sample_val_img.txt --val_coord_txt val_coordinates.txt --K 900 --compress --order xzy --gauss 0.8 --contrastive --dataset semiclass --down_ratio 1 --ge\n</code></pre> The main differences are:</p> <ul> <li><code>main_class.py</code> this is the main file to train the model for lightweight </li> <li><code>semiclass</code> this is the task name for using lightweight model (required input after <code>main_class.py</code>)</li> <li><code>--arch</code> for lightweight model the architecture name is small_5 </li> <li><code>--dataset</code> for lightweight model the dataset mode is semiclass</li> <li><code>--down_ratio</code> for lightweight model the down_ratio is 1 </li> <li><code>--ge</code> for lightweight model we use a slightly modified loss function therefore please make sure this argument is on </li> </ul> <p>Other arguments are similar to the regular model. </p>"},{"location":"refine/#inference","title":"Inference","text":""},{"location":"refine/#inference-on-globular-shaped-proteins","title":"Inference on globular shaped proteins","text":"<p>Once training is finished, we can use the trained model for testing. <code>test_img.txt</code> that contains all tomograms can be generated using <code>generate_train_files.py</code> following similar process. To run inference on all tomograms, run: <pre><code>python test.py semi --arch unet_5 --dataset semi --exp_id sample_refinement --load_model exp/semi/sample_refinement/model_4.pth --down_ratio 2 --K 900 --ord xzy --out_thresh 0.2 --test_img_txt test_img.txt --compress --gauss 0.8 --out_id all_out\n</code></pre> Outputs are saved to <code>exp/semi/sample_refinement/all_out/</code> folder. For each tomogram, 2 outputs are generated:</p> <ul> <li><code>.txt</code> file with coordinates in x,z,y order, if <code>--with_score</code> in command, a score column will be added to the output </li> <li><code>*hm.mrc</code> a 3D detection heatmap for each tomogram. </li> </ul> Arguments Purpose <code>load_model</code> path to trained model, the command here is using model from 4<sup>th</sup> epoch <code>out_thresh</code> cutoff score for detection <code>out_id</code> folder to save all outputs <code>ord</code> order of the tomogram <code>with_score</code> whether generated output will include score in addition to x,y,z coordinates <p>Warning</p> <p>Make sure to use same <code>--last_k</code>, <code>--gauss</code>, <code>--arch</code> to make sure correct loading of models.</p>"},{"location":"refine/#inference-on-tubular-shaped-proteins","title":"Inference on tubular shaped proteins","text":"<p>For tubular shaped proteins, similarly, we add <code>--fiber</code> to the command. In addition, we specify cutoff threshold for curve fitting to the infenrece command. To run inference on tomograms with fiber-specific post-processing, run:</p> <p><pre><code>python test.py semi --arch unet_5 --dataset semi --exp_id fib_test --load_model exp/semi/fib_test/model_10.pth --down_ratio 2 --K 550 --order xzy --out_thresh 0.205 --test_img_txt sample_train_microtubule_img.txt --compress --gauss 1 --cutoff_z 10 --out_id new_out --last_k 5 --fiber --curvature_cutoff 0.03 --nms 3\n</code></pre> | Arguments   | Purpose                                                                       | |:-------------|:------------------------------------------------------------------------------| | <code>curvature_cutoff</code> | max curvature for fitted curve, for microtubules the curvature should be small. Curves above the threshold will be discarded.| | <code>r2_cutoff</code> | max residual for fitted curve, discard if above the residual/bad fitting| | <code>distance_cutoff</code> | distance cutoff for whether two points are connected in graph|</p> without post-processingafter post-processing <p></p> <p></p>"},{"location":"refine/#inference-with-lightweight-model","title":"Inference with lightweight model","text":"<p>For the lightweight model, the inference command is similar to the standard model. The main differences in arguments correspond to those used in the training command. Below is a sample command for training the lightweight model:</p> <p><pre><code>python test_class.py semiclass --exp_id sample_lightweight --arch small_5 --dataset semiclass --K 900 --compress --order xzy --gauss 0.8 --load_model exp/semiclass/sample_lightweight/model_4.pth --out_thresh 0.5 --test_img_txt test_img.txt --out_id all_out --down_ratio 1 --nms 15\n</code></pre> Note, the python code for inference is <code>test_class.py</code>. Output format is the same as the standard model. </p>"},{"location":"refine/#convert-output-txt-to-mod-for-imod","title":"Convert output txt to <code>.mod</code> for imod","text":"<p>Make usre output txt files do not include score. Then, run: <pre><code>point2model -scat -sphere 5 output.txt output.mod\n</code></pre> More details on imod command. </p> <p>Output coordinates from trained model can then by used for subtomogram extraction and averaging and subsequent 3D reconstruction.</p>"}]}